# Split the data by using the 400 first observations as the training and the remaining as the testing data
train = 1:0.75*nrow(Boston)
test = -train
train
test
# STEP 1
# Split the data by using the 400 first observations as the training and the remaining as the testing data
train = 1:(0.75*nrow(Boston))
library(MASS)
dim(Boston)
names(Boston)
# STEP 1
# Split the data by using the 400 first observations as the training and the remaining as the testing data
train = 1:(0.75*nrows(Boston))
test = -train
train
test
# STEP 3 : Linearity check between medv and age
cor(Boston$medv, Boston$age)
cor(Boston$medv, Boston$age)
plot(training_data$age,
training_data$medv,
main="Households with age in function of median house value",
xlab="Age",
ylab="Median Value",
pch=0,
col='green',
cex=1.2)
# STEP 4 :Fit linear model between medv and age, with plots
variables = which(names(Boston) == c("age", "medv"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
plot(training_data$age,
training_data$medv,
main="Households with age in function of median house value",
xlab="Age",
ylab="Median Value",
pch=0,
col='green',
cex=1.2)
# STEP 4 :Fit linear model between medv and age, with plots
variables = which(names(Boston) == c("age", "medv"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
model <- lm(formula = medv ~ age, data = training_data)
plot(training_data$age,
training_data$medv,
main="Households with age in function of median house value",
xlab="Age",
ylab="Median Value",
pch=3,
col='blue',
cex=1.2)
abline(model, col='red')
variables = which(names(Boston) == c("age", "medv", "lstat"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
model <- lm(formula = medv ~ age + log(lstat), data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
# STEP 5 : Fit linear model with lstat and age as predictors
variables = which(names(Boston) == c("age", "medv", "lstat"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
model <- lm(formula = medv ~ age + log(lstat), data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
names(Boston)
names(Boston)
# STEP 5 : Fit linear model with lstat and age as predictors
variables = which(names(Boston) == c("age", "medv", "lstat"))
names(Boston)
# STEP 5 : Fit linear model with lstat and age as predictors
variables = which(names(Boston) == c("medv", "lstat"))
abline(model, col='red')
names(Boston)
# STEP 5 : Fit linear model with lstat and age as predictors
variables = which(names(Boston) == c("medv", "lstat", "age"))
names(Boston)
# STEP 5 : Fit linear model with lstat and age as predictors
variables = which(names(Boston) == c("medv", "lstat"))
names(Boston)
# STEP 5 : Fit linear model with lstat and age as predictors
varNames = c("lstat", "medv", "age")
training_data = Boston[train, varNames]
testing_data = Boston[test, varNames]
View(testing_data)
View(training_data)
# STEP 5 : Fit linear model with lstat and age as predictors
varNames = c("lstat", "medv", "age")
training_data = Boston[train, varNames]
testing_data = Boston[test, varNames]
model <- lm(formula = medv ~ age + log(lstat), data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
# STEP 5 : Fit linear model with lstat and age as predictors
varNames = c("lstat", "medv", "age")
training_data = Boston[train, varNames]
testing_data = Boston[test, varNames]
model <- lm(formula = medv ~ age + log(lstat), data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
anova(model)
# STEP 9 : Train with all variables
training_data = Boston[train, .]
testing_data = Boston[test, .]
model <- lm(formula = medv ~ ., data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
training_data = Boston[train, 1:ncols(Boston)]
testing_data = Boston[test, 1:ncols(Boston)]
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
# STEP 9 : Train with all variables
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ., data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
# STEP 10 : Train with all variables (log(lstat))
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
# R²_adj is 0.7197
# STEP 10 : Train with all variables (log(lstat))
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
# R²_adj is 0.7197
# STEP 6 : Summary of the model
summary(model)
# STEP 11 : Correlatio matrix
cor(model)
# STEP 11 : Correlation matrix
cor(Boston)
round(cor(Boston), 2)
# STEP 12 : Corrplot visualization
install.packages("corrplot")
library(corrplot)
library(corrplot)
corrplot(corMatrix, method="squares")
corMatrix <- round(cor(Boston), 2)
# STEP 12 : Corrplot visualization
library(corrplot)
corrplot(corMatrix, method="square")
# STEP 12 : Corrplot visualization
library(corrplot)
corrplot(corMatrix, method="number")
# STEP 14 : Cor between tax and rad
corTaxRad <- corMatrix["tax", "rad"]
# STEP 15 : Model without tax
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ((. - lstat - tax) + log(lstat)), data = training_data)
summary(model)
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ., data = training_data)
# R²_adj is 0.7197
# STEP 10 : Train with all variables (log(lstat))
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
summary(model)
# STEP 15 : Model without tax
training_data = Boston[train, 1:length(Boston)-c("tax")]
testing_data = Boston[test, 1:length(Boston)-c("tax"
training_data = Boston[train, 1:length(Boston)-which(names(Boston) == c("tax"))]
training_data = Boston[train, 1:length(Boston)-which(names(Boston) == c("tax"))]
training_data = Boston[train, which(names(Boston) != c("tax"))]
training_data = Boston[train, which(names(Boston) != c("tax"))]
testing_data = Boston[test, which(names(Boston) != c("tax"))]
model <- lm(formula = medv ~ ((. - lstat - tax) + log(lstat)), data = training_data)
summary(model)
# STEP 15 : Model without tax
training_data = Boston[train, which(names(Boston) != c("tax"))]
testing_data = Boston[test, which(names(Boston) != c("tax"))]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
summary(model)
y = testing_data$medv
y_hat = predict(model, data.frame(testing_data))
y = testing_data$medv
y_hat = predict(model, data.frame(testing_data))
error = y - y_hat
error_squared = error^2
MSE = mean(error_squared)
# STEP 16 : MSE
y = testing_data$medv
y_hat = predict(model, data.frame(testing_data$lstat))
# STEP 16 : MSE
y = testing_data$medv
y_hat = predict(model, data.frame(testing_data))
error = y - y_hat
error_squared = error^2
MSE = mean(error_squared)
names(Boston)
str(Boston$chas)
str(Boston$chas == 1)
str(Boston$chas) # vector of logical ints (0 or 1)
length(Boston$chas[Boston$chas == 1])
# STEP 17 : Exploring charles river variables
str(Boston$chas) # vector of logical ints (0 or 1)
length(Boston$chas[Boston$chas == 1]) / length(Boston$chas)
boxplot(Boston$medv ~ Boston$chas)
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
main="Media house value with respect to chas"
)
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
names=c("River", "Not river"),
main="Media house value with respect to chas"
)
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
names=c("Not bounding", "Bounds to river"),
main="Media house value with respect to chas"
)
# STEP 18 : Boxplot of medv with chas
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
names=c("Not bounding", "Bounds to river"),
ylab="Medv"
main="Media house value with respect to chas"
)
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
names=c("Not bounding", "Bounds to river"),
ylab="Medv",
main="Media house value with respect to chas"
)
Mu_i <- mean(aggregate(x=Boston$medv, by=Boston$chas[Boston$chas == 0]))
Mu_i <- mean(aggregate(x=Boston$medv, by=Boston$chas[Boston$chas == 0], FUN=length))
Mu_i <- mean(aggregate(x=Boston$medv, by=list(Boston$chas[Boston$chas == 0]), FUN=length))
Mu_i <- mean(aggregate(x=Boston$medv, by=list(Boston$chas), FUN=length))
# STEP 16 : MSE
y = testing_data$medv
# STEP 19 : Mui and Muj
Mu_i <- aggregate(x=Boston$medv, by=list(Boston$chas), FUN=length)
View(Mu_i)
Mu_i <- aggregate(x=Boston$medv, by=list(Boston$chas), FUN=mean)
View(Mu_i)
means <- aggregate(x=Boston$medv, by=list(Boston$chas), FUN=mean)
Boston
fit <- aov(Boston$medv ~ Boston$chas, data=Boston)
summary(fit)
names(Boston)
training_data = Boston[train, which(names(Boston) == c("crim", "chas"))]
testing_data = Boston[test, which(names(Boston) == c("crim", "chas"))]
training_data = Boston[train, which(names(Boston) == c("crim", "chas"))]
testing_data = Boston[test, which(names(Boston) == c("crim", "chas"))]
model <- lm(formula = medv ~ crim + chas), data = training_data)
summary(model)
# STEP 21 : Model with qualitative predcitors
training_data = Boston[train, which(names(Boston) == c("crim", "chas"))]
testing_data = Boston[test, which(names(Boston) == c("crim", "chas"))]
model <- lm(formula = medv ~ crim + chas, data = training_data)
summary(model)
# STEP 21 : Model with qualitative predcitors
training_data = Boston[train, which(names(Boston) == c("crim", "chas"))]
testing_data = Boston[test, which(names(Boston) == c("crim", "chas"))]
model <- lm(formula = medv ~ crim + chas, data = training_data)
summary(model)
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ crim + chas, data = training_data)
summary(model)
training_data = Boston[train, which(names(Boston) != c("tax"))]
testing_data = Boston[test, which(names(Boston) != c("tax"))]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
summary(model)
# STEP 23 : Interaction terms
model <- lm(formula = medv ~ lstat * age, data = training_data)
summary(model)
# STEP 24 : All interaction terms model
model <- lm(formula = medv ~ .*, data = training_data)
summary(model)
# STEP 24 : All interaction terms model
model <- lm(formula = medv ~ *, data = training_data)
summary(model)
names(Boston)
model <- lm(formula = medv ~ crim * , data = training_data)
summary(model)
names(Boston)
model <- lm(formula = medv ~ crim * zn * indus * chas * now * rm * age * dis * rad * tax * ptratio * black * lstat, data = training_data)
summary(model)
names(Boston)
model <- lm(formula = medv ~ crim * zn * indus * chas * now * rm * age * dis * rad * tax * ptratio * black * lstat, data = training_data)
summary(model)
model$coefficients
names(Boston)
model <- lm(formula = medv ~ crim * zn * lstat, data = training_data)
summary(model)
model$coefficients
# STEP 24 : All interaction terms model
names(Boston)
model <- lm(formula = medv ~ (.)^2, data = training_data)
summary(model)
model$coefficients
## PWC 2 ##
# MULTIVARIATE LINEAR REGRESSION
# STEP 1 : Load dataset
library(MASS)
dim(Boston)
names(Boston)
# STEP 2 : Split the data by using the 75% first observations as the training data and the remaining as the testing data
train = 1:(0.75*nrows(Boston))
test = -train
# STEP 3 : Linearity check between medv and age
cor(Boston$medv, Boston$age)
# slight negative correlation
# STEP 4 : Fit linear model between medv and age, with plots
variables = which(names(Boston) == c("age", "medv"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
model <- lm(formula = medv ~ age, data = training_data)
plot(training_data$age,
training_data$medv,
main="Households with age in function of median house value",
xlab="Age",
ylab="Median Value",
pch=3,
col='blue',
cex=1.2)
abline(model, col='red')
# STEP 5 : Fit linear model with lstat and age as predictors
varNames = c("lstat", "medv", "age")
training_data = Boston[train, varNames]
testing_data = Boston[test, varNames]
model <- lm(formula = medv ~ age + log(lstat), data = training_data)
# multivariate regression model
# STEP 6 : Summary of the model
summary(model)
anova(model)
# STEP 7 AND 8 : Interpretation
# The predictors are quite significant given their p-value (age contributiong less than log(lstat))
# F stat is not close to zerro, thus we can reject the null hypothesis
# R²_adj is 0.63, which is okay, but the model is not ideal
# STEP 9 : Train with all variables
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ., data = training_data)
# R²_adj is 0.7197
# STEP 10 : Train with all variables (log(lstat))
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
summary(model)
# STEP 11 : R² comparison
# R²_adj is 0.771 => it improved
# STEP 12 : Correlation matrix
corMatrix <- round(cor(Boston), 2)
# STEP 13 : Corrplot visualization
corrplot(corMatrix, method="square")
# STEP 14 : Cor between tax and rad
corTaxRad <- corMatrix["tax", "rad"]
# 0.91
# STEP 15 : Model without tax
training_data = Boston[train, which(names(Boston) != c("tax"))]
testing_data = Boston[test, which(names(Boston) != c("tax"))]
model <- lm(formula = medv ~ ((. - lstat) + log(lstat)), data = training_data)
summary(model)
# R goes a little lower, but F-stat goes from 98.27 to 102.7 => our model is better without tax
# STEP 16 : MSE
y = testing_data$medv
y_hat = predict(model, data.frame(testing_data))
error = y - y_hat
error_squared = error^2
MSE = mean(error_squared)
# STEP 17 : Exploring charles river variables
str(Boston$chas) # vector of logical ints (0 or 1)
length(Boston$chas[Boston$chas == 1]) / length(Boston$chas)
# around 7% live bounded to the river
# STEP 18 : Boxplot of medv with chas
boxplot(Boston$medv ~ Boston$chas,
col=c("blue","red"),
names=c("Not bounding", "Bounds to river"),
ylab="Medv",
main="Media house value with respect to chas"
)
# Medv is higher when house is bounding the river
# STEP 19 : Aggregating means
means <- aggregate(x=Boston$medv, by=list(Boston$chas), FUN=mean)
# STEP 20 : Variance explanation
fit <- aov(Boston$medv ~ Boston$chas, data=Boston)
summary(fit)
# F is not small thus we can reject the null hypothesis and there is a correlation between chas and medv
# STEP 21 and 22 : Model with qualitative predcitors
training_data = Boston[train, 1:length(Boston)]
testing_data = Boston[test, 1:length(Boston)]
model <- lm(formula = medv ~ crim + chas, data = training_data)
summary(model)
# chas adds more information for explaining the house price than crime rate
# he's much less significant in presence of other predictors (p-value increased)
# STEP 23 : Interaction terms
model <- lm(formula = medv ~ lstat * age, data = training_data)
summary(model)
# interaction term is small but not as significant as lstat
# STEP 24 : All interaction terms model
names(Boston)
model <- lm(formula = medv ~ (.)^2, data = training_data)
summary(model)
# MULTIVARIATE LINEAR REGRESSION
# STEP 1 : Load dataset
library(MASS)
dim(Boston)
names(Boston)
# STEP 2 : Split the data by using the 75% first observations as the training data and the remaining as the testing data
train = 1:(0.75*nrows(Boston))
nrows(Boston)
test = -train
## PWC 2 ##
# MULTIVARIATE LINEAR REGRESSION
# STEP 1 : Load dataset
library(MASS)
dim(Boston)
names(Boston)
# STEP 2 : Split the data by using the 75% first observations as the training data and the remaining as the testing data
train = 1:(0.75*nrow(Boston))
nrows(Boston)
test = -train
## PWC 1 ##
# R BASICS AND LINEAR REGRESSION
# setting current working directiory to source code file location
# Basics 1
x = c(1, 7, 3, 4)
y = 100:1
x[3] + y[4]
cos(x[3]) + sin(x[2])*exp(-y[2])
# Basics 2
fisher_distribution_90 = qf(p=0.9, df1 = 1, df2 = 5)
print(fisher_distribution_90)
fisher_distribution_95 = qf(p=0.95, df1 = 1, df2 = 5)
print(fisher_distribution_95)
fisher_distribution_99 = qf(p=0.99, df1 = 1, df2 = 5)
print(fisher_distribution_99)
x <- seq(-4, 4, length= 100)
poisson <- rpois(100, lambda = 5)
plot(x, poisson)
t1 <- pt(q = x, df=1)
plot(x, t1)
t5 <- pt(q = x, df=5)
t10 <- pt(q = x, df=10)
t50 <- pt(q = x, df=50)
t100 <- pt(q = x, df=100)
lines(x, t5, col='green')
lines(x, t10, col='blue')
lines(x, t50, col='red')
lines(x, t100, col='yellow')
# LINEAR REGRESSION
load("EU.RData")
summary(EU)
myModel <- lm(formula = CamCom2011 ~ Population2010, data = EU)
myModel
names(myModel)
mod$coefficients
mod$residuals
mod$sigma
summaryMyModel <- summary(mod)
summaryMyModel$sigma
# BOSTON HOUSE PREDICTION
library(MASS)
dim(Boston)
names(Boston)
# STEP 1
# Split the data by using the 400 first observations as the training and the remaining as the testing data
train = 1:400
test = -train
#we are only going to use two variables
variables = which(names(Boston) == c("lstat", "medv"))
training_data = Boston[train, variables]
testing_data = Boston[test, variables]
dim(training_data)
# STEP 2
# linearity check
plot(training_data$lstat,
training_data$medv,
main="Households with low economical status in function of median house value",
xlab="Low economical status",
ylab="Median Value",
pch=0,
col='green',
cex=1.2)
plot(log(training_data$lstat), training_data$medv)
# STEP 3
# linear model
model = lm(formula = medv ~ log(lstat), data=training_data)
model
summary(model)
confint(model, level = 0.95)
plot(log(training_data$lstat), training_data$medv,
xlab="Log transform of % of Household with low socioecomic income",
ylab="Median House Value",
col='red',
pch = 20)
abline(model, col='blue', lwd = 3)
# Prediction
predict(model, data.frame(lstat = c(5)))
predict(model, data.frame(lstat = c(5, 10, 15), interval= "prediction"))
y = testing_data$medv
y_hat = predict(model, data.frame(lstat = testing_data$lstat))
error = y - y_hat
error_squared = error^2
MSE = mean(error_squared)
MSE
## Alexandre Zajac, June 2019.
